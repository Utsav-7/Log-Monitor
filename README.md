# Log Monitor

This script monitors a Python application's log output in real-time, parsing and analyzing log messages to count occurrences of different log levels and error messages.

## **Requirements**

- Python 3.x

## **Project Structure overview**

This project contain the three files :

1) constant.py

- Included all static variables that needed in [app.py](http://app.py) and monitor.py
- Main reason for separate file is to build single source of truth for monitoring system

2) app.py

- Dummy application that constantly logged different different log messages

3) monitor.py

- Code written to extract constantly generated messages from [app.py](http://app.py) to analyze further
- Main three three kinds of analysis were done:
    - Counting DEBUG, WARNING and INFO log messages at particular time duration
    - Counting DEBUG, WARNING and INFO log by every path at particular time duration
    - Most hit API path in last particular time duration

## **Code Explanation**

### **index.py**

- The following line configures the logging module to set the root logger level to DEBUG.

```jsx
logging.basicConfig(level=logging.DEBUG)
```

- It defines a list of API paths those are implemented in and user  might interact with.

```jsx
api_paths = AVAILABLE_API_PATH
```

- Defined the format  and log level to generate random log message

```python
# Define log message formats
formats = {
    logging.INFO: "INFO message",
    logging.DEBUG: "DEBUG message",
    logging.ERROR: "ERROR message",
}

# Define log levels to cycle through
log_levels = [logging.INFO, logging.DEBUG, logging.ERROR]
```

- Log messages are randomly generated with different log levels (INFO, DEBUG, ERROR) and include a randomly chosen API path.
- Main loop logic, which first randomly generates a message and log the message every one seconds until keyboard interruption occurred such as Ctrl + C or exception occurred

```
# Main loop to log messages
while True:
    try:
        # Randomly select a log level and, path
        log_level = random.choice(log_levels)

        path = random.choice(api_paths)
        
        # Get the log message format for the selected log level
        log_message = formats[log_level] + f" for {path}"
        
        # Log the message
        logger.log(log_level, log_message)

        time.sleep(1)

    except KeyboardInterrupt:
        
        # Handle keyboard interrupt (Ctrl+C)
        print("\nLogging interrupted. Exiting.")
        break
    
    except Exception as e:
        warnings.warn(f"Error occurred: {e}")
```

### **monitor.py**

- **Subprocess Creation**: The  monitor_application function is designed to monitor log messages generated by [app.py](http://app.py) constantly and then analyse further by parsing each message
    - In order to catch log messages generated by app.py, here i used subprocess , which executes  given command.
    - The subprocess is configured to capture both standard output (stdout) and standard error (stderr) streams, redirecting them to the same pipe. This is achieved by setting `stdout=subprocess.PIPE` and `stderr=subprocess.STDOUT`
    - The `universal_newlines=True` argument ensures that the output is read as text instead of bytes,
- **Initialization of Counters**: Initialized the counter to keep track the number of messages analyzed (Here i assume that log that every second generate is analyzed within milliseconds). These counters are used to track the number of log messages analyzed and the number of times the most frequently hit path is analyzed, respectively.
- **Log Monitoring Loop**: The function enters an infinite loop where it continuously reads lines from the subprocess's stdout. For each line read, it attempts to parse the log message using a hypothetical `parse_log_message()` function. If the parsing is successful, it updates counters and possibly prints analysis results or warnings if the parsing fails.
- **Analysis and Reporting**:
    - If the `overall_analysis_count` reaches a overall analyze duration  (`OVERALL_ANALYSIS_DURATION`), the function calls `print_overall_analysis()` to print the analysis results and resets the counter.
    - If the `max_path_hit_analysis_count` reaches max API hit analyze duration (`MAX_PATH_HIT_ANALYSIS_DURATION`), it identifies the most frequently hit path using `find_most_hit_path(path_level_counts)` and prints a message indicating the most hit API in the last analysis duration.
    - If the current time and last write has a certain time duration (here 1 hour) , the process of writing data to a CSV file will be done and csv file will be generated in current folder
- **Error Handling**: The function includes error handling for keyboard interrupts (Ctrl+C) and other exceptions. On a keyboard interrupt, it stops the monitoring loop, prints a message, and terminates the subprocess. For other exceptions, it prints a warning message.

```
def monitor_application():
    global level_counts, path_level_counts, last_write_time # Declare these as global

    try:
        # Start the application as a subprocess
        application_command = ["python", "app.py"]
        application_process = subprocess.Popen(application_command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, universal_newlines=True)
       
        overall_analysis_count = 0
        max_path_hit_analysis_count = 0
        # Continuously read and analyse application's log messagess
        while True:
            line = application_process.stdout.readline().strip()
            print("/////////////////////////////////////// LINE",line)
            if line:
                parsed = parse_log_message(line.strip())

                if parsed:
                    level, log_location, error_message, path = parsed
                    level_counts[level] += 1
                    path_level_counts[path][level] += 1
                    overall_analysis_count+=1
                    max_path_hit_analysis_count+=1
                else:
                    print("\nXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX")
                    warnings.warn("FAILED TO PARSE FOLLOWING LOG :", line)
                    print("XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX")

                if (overall_analysis_count == OVERALL_ANALYSIS_DURATION):
                    # Print analysis results
                    print_overall_analysis()                    
                    overall_analysis_count = 0
                
                if(max_path_hit_analysis_count == MAX_PATH_HIT_ANALYSIS_DURATION):
                    max_hitted_path = find_most_hit_path(path_level_counts)
                    print("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")
                    print(f"MAXIMUM HITTED API IN LAST {MAX_PATH_HIT_ANALYSIS_DURATION} {DURATION_UNIT} IS : {max_hitted_path}")
                    print("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")

                # Check if an hour has passed since the last write
                current_time = datetime.datetime.now()
                if (current_time - last_write_time).total_seconds() >= EXPORT_CSV_DURATION:
                    write_to_csv(level_counts, path_level_counts)
                    # Reset counters and update the last write time
                    level_counts = {"info": 0, "debug": 0, "warning": 0}
                    path_level_counts = {}
                    last_write_time = current_time
                    print('???????????????????????????????????????????')

    except KeyboardInterrupt:
        # Handle Ctrl+C to stop the monitoring loop
        print("\nMonitoring stopped.")

        # Terminate the application process
        application_process.terminate()

    except Exception as e:
        warnings.warn(f"Error occurred: {e}")
```

- **Writing to csv file :**
    - write_to_csv() function writes two sets of data to a CSV file: a summary of counts by log level and a detailed breakdown of counts by path and log level. The use of `csv.DictWriter` makes it straightforward to write dictionaries to a CSV file, ensuring that the data is organized and easy to read
    
    ```
    def write_to_csv(level_counts, path_level_counts):
        # Create a filename based on the current time
        filename = f"log_counts_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        
        with open(filename, 'w', newline='') as csvfile:
            fieldnames = ['Level', 'Count']
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            
            writer.writeheader()
            for level, count in level_counts.items():
                writer.writerow({'Level': level, 'Count': count})
            
            # Write path-wise counts
            writer.writeheader()
            for path, counts in path_level_counts.items():
                for level, count in counts.items():
                    writer.writerow({'Level': f"{path} - {level}", 'Count': count})
    ```
    

## Performance Consideration

- The code uses `readline()` to read the subprocess's output line by line, which is a common approach for real-time monitoring of subprocess output. This method ensures that the monitoring loop can process the output as it becomes available, without waiting for the subprocess to complete.

## Future Work

- In the I am planning to make another file named export which will generate parquet or another compressed file format of every log and uploaded on cloud based storage such aws s3 bucket.
- Also want to upgrade the summary and exporting it into csv file every day and send email to respective person.
